diff --git a/docs/examples/distributed/002_multi_gpu/main.py b/docs/examples/distributed/003_multi_node/main.py
index 9cc7ef1..a703273 100644
--- a/docs/examples/distributed/002_multi_gpu/main.py
+++ b/docs/examples/distributed/003_multi_node/main.py
@@ -2,7 +2,6 @@
 import logging
 import os

-import rich.logging
 import torch
 import torch.distributed
 from torch import Tensor, nn
@@ -23,19 +22,21 @@ def main():

     # Check that the GPU is available
     assert torch.cuda.is_available() and torch.cuda.device_count() > 0
-    rank, world_size = setup()
+    rank, world_size, local_rank = setup()
     is_master = rank == 0
-    device = torch.device("cuda", rank)
+    is_local_master = local_rank == 0
+    device = torch.device("cuda", local_rank)
+    import rich.logging

     # Setup logging (optional, but much better than using print statements)
     logging.basicConfig(
         level=logging.INFO,
         format=f"[{rank}/{world_size}] %(name)s - %(message)s ",
-        handlers=[rich.logging.RichHandler(markup=True)],  # Very pretty, uses the `rich` package.
+        handlers=[rich.logging.RichHandler(markup=True)],
     )

     logger = logging.getLogger(__name__)
-    logger.info(f"World size: {world_size}, global rank: {rank}")
+    logger.info(f"World size: {world_size}, global rank: {rank}, local rank: {local_rank}")

     # Create a model and move it to the GPU.
     model = resnet18(num_classes=10)
@@ -43,14 +44,18 @@ def main():

     # Wrap the model with DistributedDataParallel
     # (See https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel)
-    model = nn.parallel.DistributedDataParallel(model, device_ids=[rank], output_device=rank)
+    model = nn.parallel.DistributedDataParallel(
+        model, device_ids=[local_rank], output_device=local_rank
+    )

     optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

     # Setup CIFAR10
     num_workers = get_num_workers()
     dataset_path = os.environ.get("SLURM_TMPDIR", "../dataset")
-    train_dataset, valid_dataset, test_dataset = make_datasets(dataset_path, is_master=is_master)
+    train_dataset, valid_dataset, test_dataset = make_datasets(
+        dataset_path, is_master=is_local_master
+    )

     # Restricts data loading to a subset of the dataset exclusive to the current process
     train_sampler = DistributedSampler(dataset=train_dataset, shuffle=True)
@@ -201,12 +206,14 @@ def setup():
     if "SLURM_PROCID" in os.environ:
         # DDP Job is being run via `srun` on a slurm cluster.
         rank = int(os.environ["SLURM_PROCID"])
+        local_rank = int(os.environ["SLURM_LOCALID"])
         world_size = int(os.environ["SLURM_NTASKS"])

         # SLURM var -> torch.distributed vars in case needed
         # NOTE: Setting these values isn't exactly necessary, but some code might assume it's
         # being run via torchrun or torch.distributed.launch, so setting these can be a good idea.
         os.environ["RANK"] = str(rank)
+        os.environ["LOCAL_RANK"] = str(local_rank)
         os.environ["WORLD_SIZE"] = str(world_size)

         torch.distributed.init_process_group(
@@ -219,8 +226,9 @@ def setup():
         # DDP via torchrun, torch.distributed.launch
         torch.distributed.init_process_group(backend="nccl", init_method="env://")
         rank = torch.distributed.get_rank()
+        local_rank = rank % torch.cuda.device_count()
         world_size = torch.distributed.get_world_size()
-    return rank, world_size
+    return rank, world_size, local_rank


 def make_datasets(
